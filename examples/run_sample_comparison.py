"""
Quick comparison with sample mode (faster, still statistically valid)
"""

import sys
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.benchmarks import BenchmarkRunner
from llm_evaluator.visualizations import quick_comparison
from pathlib import Path

def main():
    models = ["qwen2.5:0.5b", "phi3.5:3.8b"]
    sample_size = 500  # 500 questions per benchmark (statistically valid)
    results = {}
    
    print("=" * 80)
    print("  üèÜ LLM LIGHTWEIGHT COMPARISON: Qwen2.5 vs Phi-3.5 (SAMPLE MODE)")
    print("=" * 80)
    print()
    print("üìã Configuration:")
    print(f"   Models: {', '.join(models)}")
    print(f"   Mode: Sample ({sample_size} questions per benchmark)")
    print(f"   Total: {sample_size * 3} questions per model")
    print("   Output: outputs/comparisons/")
    print()
    
    for i, model_name in enumerate(models, 1):
        print(f"\n[{i}/{len(models)}] {model_name}")
        print()
        
        # Create provider
        provider = OllamaProvider(model=model_name)
        
        # Basic evaluation
        print(f"üîç Evaluating {model_name}...")
        print("   üìä Running basic evaluation...")
        evaluator = ModelEvaluator(provider=provider)
        basic_results = evaluator.evaluate_all()
        
        # Sampled benchmarks
        print(f"\n   üéØ Running sampled benchmarks ({sample_size} questions each)...")
        benchmark_runner = BenchmarkRunner(
            provider=provider, 
            use_full_datasets=True,
            sample_size=sample_size
        )
        
        print(f"\nüß™ Running benchmarks on {model_name}...")
        print()
        
        # Run all benchmarks with progress bars
        mmlu_results = benchmark_runner.run_mmlu_sample()
        truthfulqa_results = benchmark_runner.run_truthfulqa_sample()
        hellaswag_results = benchmark_runner.run_hellaswag_sample()
        
        # Combine results
        results[model_name] = {
            "basic": basic_results,
            "mmlu": mmlu_results,
            "truthfulqa": truthfulqa_results,
            "hellaswag": hellaswag_results,
        }
        
        print(f"\n‚úÖ {model_name} completed!")
        print(f"   MMLU: {mmlu_results['mmlu_accuracy']:.1%} ({mmlu_results['questions_tested']} questions)")
        print(f"   TruthfulQA: {truthfulqa_results['truthfulness_score']:.1%} ({truthfulqa_results['questions_tested']} questions)")
        print(f"   HellaSwag: {hellaswag_results['hellaswag_accuracy']:.1%} ({hellaswag_results['scenarios_tested']} scenarios)")
        
        if 'elapsed_time' in mmlu_results:
            total_time = (mmlu_results['elapsed_time'] + 
                         truthfulqa_results['elapsed_time'] + 
                         hellaswag_results['elapsed_time'])
            print(f"   Total time: {total_time/60:.1f} minutes")
        print()
    
    # Generate comparison visualizations
    print("\n" + "=" * 80)
    print("  üìä GENERATING COMPARISON VISUALIZATIONS")
    print("=" * 80)
    
    output_dir = Path("outputs/comparisons")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Convert to format expected by quick_comparison
    comparison_results = {}
    for model, data in results.items():
        comparison_results[model] = data["basic"]
        # Add benchmark scores to detailed_metrics
        comparison_results[model].detailed_metrics.benchmarks = {
            "mmlu": data["mmlu"]["mmlu_accuracy"],
            "truthfulqa": data["truthfulqa"]["truthfulness_score"],
            "hellaswag": data["hellaswag"]["hellaswag_accuracy"],
        }
    
    quick_comparison(comparison_results, output_dir=str(output_dir))
    
    print("\n" + "=" * 80)
    print("  ‚úÖ COMPARISON COMPLETE!")
    print("=" * 80)
    print(f"\nüìä Visualizations saved to: {output_dir.absolute()}")
    print(f"üåê Open in browser: {output_dir / 'comparison_dashboard.html'}")
    print()
    
    # Print summary comparison
    print("=" * 80)
    print("  üìà FINAL COMPARISON (Sample Mode)")
    print("=" * 80)
    print()
    
    for model in models:
        data = results[model]
        print(f"{model}:")
        print(f"  Overall Score:     {data['basic'].overall_score:.2f}/1.00")
        print(f"  MMLU:              {data['mmlu']['mmlu_accuracy']:.1%} ({data['mmlu']['questions_tested']} questions)")
        print(f"  TruthfulQA:        {data['truthfulqa']['truthfulness_score']:.1%} ({data['truthfulqa']['questions_tested']} questions)")
        print(f"  HellaSwag:         {data['hellaswag']['hellaswag_accuracy']:.1%} ({data['hellaswag']['scenarios_tested']} scenarios)")
        print(f"  Token Efficiency:  {data['basic'].token_efficiency:.1f} tokens/s")
        print(f"  Avg Response Time: {data['basic'].avg_response_time:.2f}s")
        
        if 'elapsed_time' in data['mmlu']:
            total_time = (data['mmlu']['elapsed_time'] + 
                         data['truthfulqa']['elapsed_time'] + 
                         data['hellaswag']['elapsed_time'])
            print(f"  Total Benchmark Time: {total_time/60:.1f} minutes")
        print()
    
    print("\nüí° Note: These results are based on sampled benchmarks.")
    print(f"   Statistical confidence: ¬±{1.96 * (0.5 / (sample_size ** 0.5)):.1%} at 95% confidence level")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå Interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
