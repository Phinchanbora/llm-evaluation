# LinkedIn Post - LLM Benchmark Toolkit Launch

---

## Post Principal

ğŸš€ **Acabo de lanzar llm-benchmark-toolkit** - una herramienta open-source para evaluar modelos de lenguaje.

Â¿Por quÃ© la construÃ­? Porque evaluar LLMs no deberÃ­a requerir un PhD ni un presupuesto de startup.

**Lo que hace:**
â€¢ 10 benchmarks con 132K+ preguntas reales
â€¢ Security testing incluido (SafetyBench, Do-Not-Answer)
â€¢ Dashboard visual para comparar modelos
â€¢ Funciona con Ollama, OpenAI, Claude, y mÃ¡s

**Un comando:**

```
pip install llm-benchmark-toolkit
llm-eval dashboard
```

<!-- 
[IMAGEN 1: Screenshot del dashboard mostrando comparaciÃ³n de modelos]
- Mostrar: Tabla con llama3.2:1b vs gpt-4o-mini vs claude-3-haiku
- Columnas: MMLU, TruthfulQA, HellaSwag, SafetyBench
- Incluir grÃ¡fico de barras comparativo
-->

El proyecto es 100% open-source. Feedback bienvenido.

ğŸ”— GitHub: github.com/NahuelGiudizi/llm-evaluation

# OpenSource #AI #LLM #MachineLearning #Python

---

## Post Alternativo (mÃ¡s tÃ©cnico)

DespuÃ©s de meses trabajando con LLMs, me cansÃ© de:
âŒ Evaluar modelos "a ojo"
âŒ Copiar preguntas manualmente
âŒ No tener mÃ©tricas comparables

AsÃ­ que construÃ­ **llm-benchmark-toolkit**:

ğŸ“Š **132,619 preguntas** de benchmarks reales:

- MMLU (conocimiento)
- TruthfulQA (veracidad)
- HellaSwag (razonamiento)
- SafetyBench (seguridad)
- Y 6 mÃ¡s...

ğŸ›¡ï¸ **Security-first**: EvaluÃ¡ si tu modelo rechaza contenido daÃ±ino

ğŸ–¥ï¸ **Dashboard React**: VisualizÃ¡ y comparÃ¡ resultados

âš¡ **Multi-provider**: Ollama, OpenAI, Anthropic, DeepSeek

<!--
[IMAGEN 2: Terminal mostrando ejecuciÃ³n de benchmark]
- Mostrar output con barras de progreso
- Resultados por categorÃ­a (Knowledge, Reasoning, Security)
- Tiempo de ejecuciÃ³n y path de guardado
-->

Lo mejor: un `pip install` y estÃ¡s evaluando.

Link en comentarios ğŸ‘‡

---

## VersiÃ³n Corta (para engagement)

ConstruÃ­ una herramienta para evaluar LLMs con 132K preguntas reales.

Incluye benchmarks de seguridad que la mayorÃ­a ignora.

Open-source. Un comando para instalarlo.

Â¿Te interesa? Link en comentarios.

<!--
[IMAGEN 3: GrÃ¡fico mostrando accuracy por benchmark]
- Barras horizontales
- Colores por categorÃ­a (Knowledge=azul, Security=rojo, Reasoning=verde)
- Nombre del modelo arriba
-->

---

## Hashtags Recomendados

Principales: #OpenSource #AI #LLM #MachineLearning #Python
Secundarios: #DataScience #ArtificialIntelligence #TechCommunity #Programming
Argentina: #TechArgentina #ProgramacionArgentina

---

## Notas para publicar

1. **Mejor horario LinkedIn**: Martes-Jueves 8-10am o 12-2pm
2. **Primera imagen**: La mÃ¡s importante, debe captar atenciÃ³n
3. **Comentario propio**: Agregar link de GitHub inmediatamente como primer comentario
4. **Responder rÃ¡pido**: Los primeros 60 minutos son crÃ­ticos para el algoritmo
