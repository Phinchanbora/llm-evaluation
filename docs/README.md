# üìö LLM Benchmark Toolkit - Documentation

> Complete documentation for the LLM Benchmark Toolkit.

**[‚Üê Back to Main README](../README.md)**

---

## üìñ Documentation Index

| Document | Description |
|----------|-------------|
| üöÄ **[Quick Start](QUICKSTART.md)** | Installation and first evaluation in 5 minutes |
| üîå **[Providers Guide](PROVIDERS.md)** | Configure Ollama, OpenAI, Anthropic, DeepSeek, HuggingFace |
| üìò **[API Reference](API.md)** | Complete Python API documentation |
| üéì **[Academic Usage](ACADEMIC_USAGE.md)** | Statistical methods, confidence intervals, LaTeX export |
| üî¨ **[Benchmarks](FULL_BENCHMARKS.md)** | MMLU, TruthfulQA, HellaSwag details and citations |

---

## üéØ Quick Links

### Getting Started

- [Installation](QUICKSTART.md#installation)
- [First Evaluation](QUICKSTART.md#quick-commands)
- [Provider Setup](PROVIDERS.md#overview)

### Python API

- [ModelEvaluator](API.md#modelevaluator)
- [BenchmarkRunner](API.md#benchmarkrunner)
- [Providers](API.md#providers)
- [Statistical Functions](API.md#statistical-functions)

### Providers

- [Ollama (Local)](PROVIDERS.md#ollama-local)
- [OpenAI](PROVIDERS.md#openai)
- [Anthropic](PROVIDERS.md#anthropic)
- [DeepSeek](PROVIDERS.md#deepseek)
- [HuggingFace](PROVIDERS.md#huggingface)
- [Custom Provider](PROVIDERS.md#custom-provider)

### Academic Features

- [Confidence Intervals](ACADEMIC_USAGE.md#wilson-score-confidence-intervals)
- [Statistical Tests](ACADEMIC_USAGE.md#mcnemars-test)
- [Baseline Comparisons](ACADEMIC_USAGE.md#baseline-comparisons)
- [LaTeX Export](ACADEMIC_USAGE.md#latex-export)
- [BibTeX Citations](ACADEMIC_USAGE.md#bibtex-generation)

### Benchmarks

- [MMLU](FULL_BENCHMARKS.md#mmlu-massive-multitask-language-understanding)
- [TruthfulQA](FULL_BENCHMARKS.md#truthfulqa)
- [HellaSwag](FULL_BENCHMARKS.md#hellaswag)

---

## üîß Version

Current version: **v2.1.0**

- Full mypy --strict compliance
- 80% test coverage (602 tests)
- 5 providers: Ollama, OpenAI, Anthropic, DeepSeek, HuggingFace
- 24,901 benchmark questions

---

## üì¶ Installation

```bash
pip install llm-benchmark-toolkit
```

---

## ü§ù Contributing

See [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines.

---

**[‚Üê Back to Main README](../README.md)**
