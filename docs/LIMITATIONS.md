# ‚ö†Ô∏è Limitations & Known Issues

## Perspectiva CTO: An√°lisis Honesto

Este documento presenta las **limitaciones reales** del proyecto desde una perspectiva enterprise.

---

# ‚ö†Ô∏è Limitations & Known Issues

## Perspectiva CTO: An√°lisis Honesto

Este documento presenta las **limitaciones reales** del proyecto desde una perspectiva enterprise.

---

## ‚úÖ RESUELTO: Benchmarks Demo ‚Üí Production-Ready

### ~~1. Benchmarks Demo (NOT Production-Ready)~~ ‚úÖ FIXED

**Estado:** ‚úÖ **RESUELTO** - Implementaci√≥n completa disponible

**Soluci√≥n Implementada:**

- ‚úÖ MMLU: **14,042 preguntas reales** desde HuggingFace
- ‚úÖ TruthfulQA: **817 preguntas reales** desde HuggingFace
- ‚úÖ HellaSwag: **10,042 escenarios reales** desde HuggingFace

**Ahora V√°lido Para:**

- ‚úÖ Investigaci√≥n acad√©mica
- ‚úÖ Comparaciones serias entre modelos
- ‚úÖ Publicaci√≥n en papers
- ‚úÖ Benchmarking enterprise

**Uso:**

```python
from llm_evaluator.benchmarks import BenchmarkRunner

# Modo demo (r√°pido, desarrollo)
runner = BenchmarkRunner(provider)

# Modo sampling (recomendado, 5-10 min)
runner = BenchmarkRunner(provider, use_full_datasets=True, sample_size=100)

# Modo full (producci√≥n, 2-8 horas)
runner = BenchmarkRunner(provider, use_full_datasets=True)

results = runner.run_all_benchmarks()
```

**Documentaci√≥n:** Ver `FULL_BENCHMARKS.md` y `CHANGELOG_BENCHMARKS.md`

**Fecha de Resoluci√≥n:** Noviembre 29, 2025

---

## üî¥ Limitaciones Cr√≠ticas (Bloqueantes para Producci√≥n)

### 1. Solo Ollama Provider Implementado

**Problema:**

- OpenAI provider: **NO IMPLEMENTADO**
- Anthropic provider: **NO IMPLEMENTADO**
- HuggingFace provider: **NO IMPLEMENTADO**

**Impacto:**

- Limitado a modelos locales Ollama
- No puede comparar GPT-4, Claude, etc.
- Promesas de "multi-provider" son arquitectura, no realidad

**Soluci√≥n:**

```python
# Implementar providers faltantes
class OpenAIProvider(LLMProvider):
    def generate(self, prompt, config):
        # API call a OpenAI
        pass
```

**Esfuerzo:** 1-2 d√≠as por provider

**Status:** üèóÔ∏è Arquitectura lista, implementations faltantes

---

### 2. Performance con Modelos Grandes (>7B)

**Problema:**

- Modelos 33B+: **muy lentos** sin GPU
- Timeouts frecuentes
- Memoria insuficiente en m√°quinas normales

**Datos Reales:**

```
llama3.2:1b  ‚Üí 0.5s por prompt  ‚úÖ
mistral:7b   ‚Üí 2-3s por prompt  ‚ö†Ô∏è
deepseek:33b ‚Üí 15-30s por prompt ‚ùå
```

**Impacto:**

- Evaluaciones de modelos grandes toman **horas**
- No escalable para CI/CD
- Necesita hardware especializado

**Soluci√≥n:**

- Configurar timeouts adecuados
- Usar GPU (CUDA/Metal)
- Batch processing (parcialmente implementado)
- Considerar vLLM para inference r√°pido

**Status:** ‚ö†Ô∏è Limitaci√≥n inherente de LLMs locales

---

### 3. Coverage Gaps

**Tests Coverage:**

- Core modules: 95%+
- Visualizations: **70%** (matplotlib dif√≠cil de testear)
- End-to-end: **0%** (no integration tests)

**Problema:**

```python
# Sin tests:
- Ollama connection failures
- Network timeouts reales
- Concurrent evaluations
- Large model memory issues
```

**Impacto:**

- Bugs en edge cases
- No confidence en refactors grandes
- CI/CD incompleto

**Soluci√≥n:**

```bash
# Agregar integration tests
pytest tests/integration/ --slow
```

**Esfuerzo:** 3-5 d√≠as

**Status:** üî¥ Gap significativo

---

## üü° Limitaciones Importantes (No Bloqueantes)

### 5. M√©tricas de Calidad Simplificadas

**Problema:**

```python
# Actual: substring matching
if "Paris" in response.lower():
    correct += 1

# Deber√≠a: semantic similarity
similarity = sentence_transformers.util.cos_sim(expected, response)
```

**Impacto:**

- False positives/negatives
- No captura respuestas equivalentes
- Ejemplo: "Par√≠s" vs "Paris" ‚Üí falla

**Soluci√≥n:**

```python
pip install sentence-transformers
# Usar embeddings para similarity
```

**Esfuerzo:** 1-2 d√≠as

---

### 6. No Database / No Hist√≥rico

**Problema:**

- Resultados solo en archivos `.md`
- No tracking de evoluci√≥n temporal
- No comparaci√≥n hist√≥rica
- No analytics

**Impacto:**

```python
# Quiero ver:
"¬øEl modelo mejor√≥ desde la semana pasada?"
"¬øQu√© configuraci√≥n funciona mejor?"

# Actual:
Comparar 50 archivos .md manualmente üò≠
```

**Soluci√≥n:**

```python
# Agregar SQLite/PostgreSQL
from sqlalchemy import create_engine
results.save_to_db()

# O timeseries DB
influxdb.write(results)
```

**Esfuerzo:** 3-4 d√≠as

---

### 7. Hallucination Detection B√°sico

**Problema:**

```python
# Actual: keyword matching
uncertainty_markers = ["don't know", "not sure"]
if any(marker in response):
    no_hallucination = True

# D√©bil para:
- Hallucinations sutiles
- Confabulaci√≥n creativa
- Facts incorrectos con alta confianza
```

**Impacto:**

- Hallucination rate **no confiable**
- Falsos negativos frecuentes

**Soluci√≥n:**

- Integrar fact-checking APIs
- RAG con knowledge base
- Modelo especializado para validaci√≥n

**Esfuerzo:** 1 semana+

---

### 8. Configuraci√≥n Pydantic No Usada en C√≥digo

**Problema:**

```python
# Implementado:
class EvaluatorConfig(BaseSettings):
    default_temperature: float = 0.7

# Pero c√≥digo NO lo usa:
evaluator = ModelEvaluator()  # Ignora config
```

**Impacto:**

- Config file es decorativo
- ENV vars no tienen efecto
- Necesita wiring manual

**Soluci√≥n:**

```python
# En evaluator.py
from llm_evaluator.config import get_evaluator_config
config = get_evaluator_config()
self.temperature = config.default_temperature
```

**Esfuerzo:** 2-3 horas

**Status:** üü° Quick win

---

## üü¢ Limitaciones Menores (Nice to Have)

### 9. No Streaming Responses

**Problema:**

- Evaluaciones largas: sin feedback
- Usuario ve "..." por minutos
- No progress bars

**Soluci√≥n:**

```python
from tqdm import tqdm
for prompt in tqdm(prompts):
    evaluate(prompt)
```

**Esfuerzo:** 1 hora

---

### 10. No CLI Tool

**Problema:**

```bash
# No existe:
llm-eval compare llama3.2:1b mistral:7b

# Tiene que escribir Python
```

**Soluci√≥n:**

```python
# Agregar click/typer
import click

@click.command()
@click.argument('models', nargs=-1)
def compare(models):
    # ...
```

**Esfuerzo:** 1 d√≠a

---

### 11. Dependencias Pesadas

**Problema:**

```
matplotlib, seaborn, plotly, pandas, numpy, scipy
‚Üí 500MB+ de dependencias
```

**Impacto:**

- Docker images grandes
- Install time lento
- Overkill para uso b√°sico

**Soluci√≥n:**

```toml
[tool.poetry.extras]
viz = ["matplotlib", "plotly", "seaborn"]
ml = ["scikit-learn", "scipy"]
```

**Esfuerzo:** 2 horas

---

### 12. No Async/Paralelo

**Problema:**

```python
# Sequential:
for model in models:
    evaluate(model)  # 10 min cada uno ‚Üí 50 min total

# Podr√≠a ser:
asyncio.gather(*[evaluate(m) for m in models])  # ‚Üí 10 min total
```

**Impacto:**

- Comparaciones multi-modelo lentas
- No aprovecha cores

**Soluci√≥n:**

```python
async def evaluate_async(provider):
    # ...

await asyncio.gather(*evaluations)
```

**Esfuerzo:** 2-3 d√≠as

---

## üìä Scorecard Realista

| Aspecto              | Score | Comentario                    |
| -------------------- | ----- | ----------------------------- |
| **Arquitectura**     | 9/10  | Clean, SOLID, testable        |
| **Type Safety**      | 9/10  | Sin Any types, strict         |
| **Tests**            | 7/10  | 40/40 unit, 0 integration     |
| **Documentation**    | 8/10  | README bueno, falta ADRs      |
| **Benchmarks**       | 3/10  | Solo demos, no reales         |
| **Performance**      | 6/10  | Ok para 1-7B, malo para 33B+  |
| **Features**         | 7/10  | Core s√≥lido, faltan providers |
| **Production Ready** | 5/10  | ‚ö†Ô∏è No para research serio     |

**Overall: 6.75/10** - Excelente base, necesita features adicionales para enterprise

---

## üéØ Roadmap Sugerido (CTO View)

### Sprint 1 (Alta Prioridad) - 1 semana

1. ‚úÖ Integrar datasets reales (MMLU, TruthfulQA, HellaSwag)
2. ‚úÖ Wire config con c√≥digo actual
3. ‚úÖ Agregar progress bars

### Sprint 2 (Media Prioridad) - 1 semana

4. ‚úÖ OpenAI provider implementation
5. ‚úÖ Integration tests suite
6. ‚úÖ Semantic similarity para accuracy

### Sprint 3 (Baja Prioridad) - 1 semana

7. ‚úÖ Database para resultados
8. ‚úÖ CLI tool
9. ‚úÖ Async/parallel evaluation

### Post-MVP

- Anthropic provider
- Hallucination detection avanzado
- Web dashboard
- CI/CD pipeline completo

---

## üí° Recomendaciones para Uso Actual

**‚úÖ √ösalo para:**

- Prototipar comparaciones de modelos
- Learning/teaching LLM evaluation
- Demostrar arquitectura Clean
- Portfolio t√©cnico

**‚ùå NO usar para:**

- Research papers (benchmarks demo)
- Production model selection cr√≠tica
- Publicaciones acad√©micas
- Marketing sin disclaimers

**‚ö†Ô∏è Usar con disclaimers:**

- "Benchmarks demo, no producci√≥n"
- "Arquitectura enterprise-ready, features WIP"
- "Necesita integraci√≥n con datasets reales"

---

## ü§ù Transparencia = Confianza

Este documento existe porque:

1. **Honestidad t√©cnica** es esencial
2. **No sorprender** a futuros usuarios/colaboradores
3. **Roadmap claro** para llegar a 10/10
4. **Portfolio profesional** = c√≥digo + an√°lisis cr√≠tico

**En resumen:** Excelente arquitectura, necesita completar features para ser production-ready.

---

_Last updated: 2025-11-29_
_Maintainer: @NahuelGiudizi_
