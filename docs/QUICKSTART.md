# 游 Quick Start Guide

## Instalaci칩n R치pida

```bash
# 1. Clonar repositorio
git clone https://github.com/NahuelGiudizi/llm-evaluation.git
cd llm-evaluation

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -e .

# 4. Verificar instalaci칩n
python -m pytest tests/ -v
```

## Prerequisitos

**Ollama debe estar corriendo:**
```bash
# Instalar Ollama: https://ollama.ai
ollama serve  # Iniciar servidor

# Descargar modelos
ollama pull llama3.2:1b
ollama pull mistral:7b
```

## 游꿢 Casos de Uso

### 1. Evaluaci칩n B치sica (Demo R치pido)

```bash
python examples/demo.py
```

**Output:**
- `evaluation_report.md` - Reporte completo
- M칠tricas en consola

### 2. Evaluaci칩n Personalizada

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.providers import GenerationConfig

# Configurar
config = GenerationConfig(
    temperature=0.7,
    max_tokens=500,
    timeout_seconds=60,
    retry_attempts=5
)

provider = OllamaProvider(model="mistral:7b", config=config)
evaluator = ModelEvaluator(provider=provider)

# Evaluar
results = evaluator.evaluate_all()

print(f"Overall Score: {results.overall_score:.2f}")
print(f"Accuracy: {results.accuracy:.1%}")
print(f"Response Time: {results.avg_response_time:.2f}s")
```

### 3. Comparar M칰ltiples Modelos

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider

models = ["llama3.2:1b", "mistral:7b", "phi3:3.8b"]
results = {}

for model_name in models:
    print(f"\n游늵 Evaluating {model_name}...")
    provider = OllamaProvider(model=model_name)
    evaluator = ModelEvaluator(provider=provider)
    results[model_name] = evaluator.evaluate_all()

# Comparar
for name, result in results.items():
    print(f"{name}: {result.overall_score:.2f}/1.00")
```

### 4. Solo Benchmarks

```python
from llm_evaluator.benchmarks import BenchmarkRunner
from llm_evaluator.providers.ollama_provider import OllamaProvider

provider = OllamaProvider(model="llama3.2:1b")
runner = BenchmarkRunner(provider=provider)

# Individual
mmlu = runner.run_mmlu_sample()
print(f"MMLU Accuracy: {mmlu['mmlu_accuracy']:.1%}")

# Todos
all_benchmarks = runner.run_all_benchmarks()
print(f"Aggregate: {all_benchmarks['aggregate_benchmark_score']:.1%}")
```

### 5. Solo Visualizaciones

```python
from llm_evaluator.visualizations import quick_comparison

results = {
    "llama3.2:1b": {
        "mmlu": 0.65,
        "accuracy": 0.75,
        "coherence": 0.82,
        "speed": 0.90
    },
    "mistral:7b": {
        "mmlu": 0.78,
        "accuracy": 0.85,
        "coherence": 0.88,
        "speed": 0.70
    }
}

quick_comparison(results, output_dir="visualizations")
```

**Genera 7 tipos de gr치ficos:**
- Bar charts (est치ticos)
- Interactive plotly (HTML)
- Radar charts
- Heatmaps
- Trend analysis
- Box plots
- Dashboards

### 6. Evaluaci칩n Personalizada

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider

provider = OllamaProvider(model="llama3.2:1b")
evaluator = ModelEvaluator(provider=provider)

# Test set personalizado
custom_tests = [
    {"prompt": "쮺u치l es la capital de Argentina?", "expected": "Buenos Aires"},
    {"prompt": "쮺u치nto es 15 * 8?", "expected": "120"},
    {"prompt": "쯈ui칠n escribi칩 Don Quijote?", "expected": "Cervantes"}
]

metrics = evaluator.evaluate_quality(test_set=custom_tests)
print(f"Custom Accuracy: {metrics['accuracy']:.1%}")
```

## 丘뙖잺 Configuraci칩n

### Opci칩n 1: Variables de Entorno

```bash
# .env file
LLM_EVAL_DEFAULT_MODEL=mistral:7b
LLM_EVAL_DEFAULT_TEMPERATURE=0.8
LLM_EVAL_DEFAULT_TIMEOUT=60
LLM_EVAL_LOG_LEVEL=DEBUG
LLM_EVAL_OUTPUT_DIR=my_outputs

OLLAMA_BASE_URL=http://localhost:11434
BENCHMARK_USE_DEMO_BENCHMARKS=true
```

### Opci칩n 2: C칩digo

```python
from llm_evaluator.config import get_evaluator_config

config = get_evaluator_config()
config.default_temperature = 0.9
config.log_level = "DEBUG"
```

### Opci칩n 3: Constructor

```python
from llm_evaluator.providers import GenerationConfig

config = GenerationConfig(
    temperature=0.9,
    max_tokens=1000,
    timeout_seconds=120,
    retry_attempts=5
)
```

## 游빍 Testing

```bash
# Todos los tests
python -m pytest tests/ -v

# Tests espec칤ficos
python -m pytest tests/test_evaluator.py -v
python -m pytest tests/test_benchmarks.py -v

# Con coverage
pip install pytest-cov
python -m pytest tests/ --cov=src/llm_evaluator --cov-report=html

# Ver coverage
open htmlcov/index.html  # o Start htmlcov/index.html en Windows
```

## 游늵 Benchmarks: Demo vs Real

**Importante:** Los benchmarks actuales son DEMO (3-2 preguntas).

**Para producci칩n, integrar datasets reales:**

```python
# Instalar
pip install datasets

from datasets import load_dataset

# MMLU - 14,042 preguntas
mmlu = load_dataset('cais/mmlu', 'all')

# TruthfulQA - 817 preguntas
truthfulqa = load_dataset('truthful_qa', 'generation')

# HellaSwag - 10,042 scenarios
hellaswag = load_dataset('Rowan/hellaswag')
```

## 游냍 Troubleshooting

### Ollama no responde
```bash
# Verificar que est칠 corriendo
ollama list

# Reiniciar
# Windows: reiniciar servicio
# Linux/Mac: ollama serve
```

### Modelo no encontrado
```bash
# Descargar modelo
ollama pull llama3.2:1b

# Verificar descarga
ollama list
```

### Tests fallan
```bash
# Verificar instalaci칩n
pip install -e .

# Reinstalar dependencias
pip install -r requirements.txt
```

### Import errors
```python
# Asegurar que est치s en el directorio correcto
import sys
sys.path.insert(0, 'src')
```

## 游늳 Performance Tips

1. **Modelos peque침os primero**: `llama3.2:1b` es r치pido para testing
2. **Reducir samples**: `evaluate_performance(num_samples=5)` para dev
3. **Cache resultados**: Guardar reports para no re-evaluar
4. **Usar GPU**: Ollama detecta autom치ticamente
5. **Batch prompts**: El provider usa `generate_batch()` cuando posible

## 游꿛 Visualizaciones Avanzadas

```python
from llm_evaluator.visualizations import EvaluationVisualizer

viz = EvaluationVisualizer(style="dark", figsize=(12, 8))

# Comparaci칩n est치tica
viz.create_benchmark_comparison(results, output="comparison.png")

# Interactivo
viz.create_benchmark_comparison(results, output="comparison.html", interactive=True)

# Dashboard completo
viz.create_dashboard(results, output="dashboard.html")

# Radar chart
viz.create_radar_chart(results, output="radar.png")

# Heatmap
viz.create_model_heatmap(results, output="heatmap.png")
```

## 游댌 Debugging

```python
import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# O en config
from llm_evaluator.config import get_evaluator_config
config = get_evaluator_config()
config.log_level = "DEBUG"
config.log_file = "debug.log"
```

## 游닇 Pr칩ximos Pasos

1. **Explorar ejemplos**: Ver `demo.py` y `demo_visualizations.py`
2. **Leer tests**: `tests/` muestra todos los casos de uso
3. **Revisar docs**: README.md tiene m치s detalles
4. **Integrar tu caso de uso**: Crear provider personalizado si necesario
5. **Contribuir**: Issues y PRs bienvenidos

## 游뚿 Limitaciones Conocidas

Ver `LIMITATIONS.md` para lista completa de:
- Benchmarks demo (no producci칩n)
- Performance con modelos grandes
- Dependencia de Ollama
- Coverage gaps
