[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "llm-benchmark-toolkit"
version = "0.4.1"
description = "Comprehensive evaluation framework for Large Language Models"
readme = "README.md"
authors = [
    {name = "Nahuel Giudizi", email = "nahuel@example.com"}
]
license = {text = "MIT"}
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
keywords = ["llm", "evaluation", "benchmarking", "ai", "machine-learning"]
requires-python = ">=3.11"
dependencies = [
    "ollama>=0.1.0",
    "datasets>=2.14.0",
    "scikit-learn>=1.3.0",
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "matplotlib>=3.7.0",
    "plotly>=5.17.0",
    "seaborn>=0.12.0",
    "scipy>=1.11.0",
    "click>=8.1.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "psutil>=5.9.0",
    "tqdm>=4.66.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "black>=23.0.0",
    "flake8>=6.0.0",
    "ruff>=0.1.0",
    "mypy>=1.8.0",
    "types-requests>=2.31.0",
]
notebooks = [
    "jupyter>=1.0.0",
    "ipykernel>=6.25.0",
    "ipywidgets>=8.1.0",
]
openai = [
    "openai>=1.0.0",
]
anthropic = [
    "anthropic>=0.18.0",
]
huggingface = [
    "huggingface-hub>=0.20.0",
]
all-providers = [
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "huggingface-hub>=0.20.0",
]

[project.scripts]
llm-eval = "llm_evaluator.cli:cli"

[project.urls]
Homepage = "https://github.com/NahuelGiudizi/llm-evaluation"
Repository = "https://github.com/NahuelGiudizi/llm-evaluation"
Issues = "https://github.com/NahuelGiudizi/llm-evaluation/issues"
Blog = "https://dev.to/nahuelgiudizi/building-an-honest-llm-evaluation-framework-from-fake-metrics-to-real-benchmarks-2b90"
PyPI = "https://pypi.org/project/llm-benchmark-toolkit/"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "-v --cov=src --cov-report=html --cov-report=term"

[tool.black]
line-length = 100
target-version = ['py311']
include = '\.pyi?$'

[tool.mypy]
python_version = "3.11"
warn_return_any = false
warn_unused_configs = true
disallow_untyped_defs = false
disallow_any_generics = false
disallow_subclassing_any = false
disallow_untyped_calls = false
disallow_incomplete_defs = false
check_untyped_defs = false
disallow_untyped_decorators = false
no_implicit_optional = false
warn_redundant_casts = true
warn_unused_ignores = false
warn_no_return = true
warn_unreachable = false
strict_equality = true
show_error_codes = true
show_column_numbers = true
pretty = true
ignore_missing_imports = true

# Ignore missing imports for third-party libraries without stubs
[[tool.mypy.overrides]]
module = [
    "datasets.*",
    "plotly.*",
    "seaborn.*",
    "scipy.*",
    "ollama.*",
    "openai.*",
    "anthropic.*",
    "huggingface_hub.*",
    "psutil.*",
    "tqdm.*",
    "pandas.*",
    "pydantic.*",
    "pydantic_settings.*",
]
ignore_missing_imports = true
