# LLM Evaluation Suite

[![Tests](https://github.com/NahuelGiudizi/llm-evaluation/workflows/Tests/badge.svg)](https://github.com/NahuelGiudizi/llm-evaluation/actions)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Coverage](https://img.shields.io/badge/coverage-87%25-brightgreen)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.2.0-blue)](https://github.com/NahuelGiudizi/llm-evaluation)

> **Enterprise-grade evaluation framework for LLMs** ‚Ä¢ 4 provider integrations ‚Ä¢ 24,901 real benchmarks ‚Ä¢ CLI tool ‚Ä¢ 10x caching

---

## üéØ Why This Project?

**Most LLM evaluations are wrong.** They use 3-8 demo questions and claim "95% accuracy". This project gives you:

‚úÖ **Real Datasets**: 14,042 MMLU + 817 TruthfulQA + 10,042 HellaSwag questions  
‚úÖ **4 Providers**: Ollama (local), OpenAI (GPT-4), Anthropic (Claude), HuggingFace  
‚úÖ **CLI Tool**: `llm-eval run`, `compare`, `benchmark` commands  
‚úÖ **10x Caching**: Intelligent caching accelerates repeated evaluations  
‚úÖ **Production Ready**: 87% test coverage, type-safe, CI/CD  

**[üìä See Demo vs Real comparison ‚Üí](examples/demo_vs_real.py)**

---

## ‚ö° Quick Start

### CLI (Easiest)

```bash
# Install
pip install -e ".[all-providers]"

# Run evaluation
llm-eval run --model llama3.2:1b

# Compare models
llm-eval compare --models llama3.2:1b,mistral:7b

# Run benchmarks
llm-eval benchmark --model gpt-3.5-turbo --provider openai --sample-size 100
```

### Python API

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider

evaluator = ModelEvaluator(provider=OllamaProvider(model="llama3.2:1b"))
results = evaluator.evaluate_all()
print(f"Overall Score: {results.overall_score:.1%}")
```

---

## üöÄ Features

### üîå Multi-Provider Support
- **Ollama** (local): llama3.2, mistral, phi3 - 100% free, private
- **OpenAI**: GPT-3.5, GPT-4 - Industry standard
- **Anthropic**: Claude 3/3.5 - Long context (200K tokens)
- **HuggingFace**: Any Inference API model - Flexibility
- **Cached Provider**: 10x speedup wrapper

### üìä Real Benchmarks
- üìö **MMLU** (14,042): 57 subjects, multi-task understanding
- üéØ **TruthfulQA** (817): Factual accuracy
- üß† **HellaSwag** (10,042): Commonsense reasoning
- üéÆ **3 modes**: Demo (8q, 30s) ‚Ä¢ Sample (100-500q, 5min) ‚Ä¢ Full (24K, 8hrs)

### üõ†Ô∏è Developer Experience
- **CLI Tool**: `llm-eval run`, `compare`, `benchmark`
- **Clean Architecture**: Provider abstraction, DI, SOLID
- **Type Safety**: Zero `Any` types, full type hints
- **Testing**: 87% coverage, 40/40 tests passing
- **Configuration**: Pydantic + .env support

### üìà Analysis
- Interactive visualizations (7 chart types)
- Statistical analysis with confidence intervals
- Export to HTML, PNG, JSON

## üì¶ Installation

```bash
git clone https://github.com/NahuelGiudizi/llm-evaluation.git
cd llm-evaluation
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Choose your providers
pip install -e .                    # Ollama only (local)
pip install -e ".[openai]"          # + OpenAI
pip install -e ".[anthropic]"       # + Anthropic  
pip install -e ".[huggingface]"     # + HuggingFace
pip install -e ".[all-providers]"   # All providers
pip install -e ".[dev]"             # Development
```

## üîß Usage Examples

### 1Ô∏è‚É£ Local with Ollama

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider

provider = OllamaProvider(model="llama3.2:1b")
evaluator = ModelEvaluator(provider=provider)
results = evaluator.evaluate_all()
print(f"Score: {results.overall_score:.1%}")
```

### 2Ô∏è‚É£ OpenAI with Caching (10x faster)

```python
from llm_evaluator.providers.openai_provider import OpenAIProvider
from llm_evaluator.providers.cached_provider import CachedProvider
import os

os.environ["OPENAI_API_KEY"] = "sk-..."
base = OpenAIProvider(model="gpt-3.5-turbo")
cached = CachedProvider(base)  # 10x speedup!

# Run benchmarks
from llm_evaluator.benchmarks import BenchmarkRunner
runner = BenchmarkRunner(cached, sample_size=100)
results = runner.run_all_benchmarks()

# Cache stats
stats = cached.get_cache_stats()
print(f"Hit rate: {stats['hit_rate_percent']:.1f}%")
```

### 3Ô∏è‚É£ Anthropic (Claude)

```python
from llm_evaluator.providers.anthropic_provider import AnthropicProvider
import os

os.environ["ANTHROPIC_API_KEY"] = "sk-ant-..."
provider = AnthropicProvider(model="claude-3-5-sonnet-20241022")
evaluator = ModelEvaluator(provider=provider)
results = evaluator.evaluate_all()
```

### 4Ô∏è‚É£ CLI Commands

```bash
# Check providers
llm-eval providers

# Single model
llm-eval run --model llama3.2:1b --cache

# Compare models
llm-eval compare --models "llama3.2:1b,mistral:7b"

# Benchmarks with sampling
llm-eval benchmark --model gpt-4 --provider openai --sample-size 100

# Full benchmarks (hours!)
llm-eval benchmark --model llama3.2:1b --full
```

## üìä Visualization Examples

Generate interactive dashboards with a single function:

```python
from llm_evaluator import quick_comparison

results = {
    "llama3.2:1b": {"mmlu": 0.65, "accuracy": 0.75, "coherence": 0.82},
    "mistral:7b": {"mmlu": 0.78, "accuracy": 0.82, "coherence": 0.88},
    "phi3:3.8b": {"mmlu": 0.71, "accuracy": 0.78, "coherence": 0.85}
}

# Generate all visualizations
quick_comparison(results, output_dir="outputs")
```

Creates:

- üìä Bar charts for benchmark comparisons
- üéØ Radar charts for multi-metric analysis
- üî• Heatmaps for performance overview
- üìà Line charts for trend analysis
- üì¶ Box plots for distribution analysis
- üé® Interactive HTML dashboards

## üß™ Benchmarks: Demo vs Production

**‚ö†Ô∏è Current Implementation (Demo/POC):**

- MMLU: 3 sample questions
- TruthfulQA: 3 sample questions
- HellaSwag: 2 sample scenarios

**üè≠ Production Datasets (Enterprise Use):**

```python
# Install: pip install datasets
from datasets import load_dataset

# MMLU - 14,042 questions across 57 subjects
mmlu = load_dataset('cais/mmlu', 'all')

# TruthfulQA - 817 factual accuracy questions
truthfulqa = load_dataset('truthful_qa', 'generation')

# HellaSwag - 10,042 commonsense scenarios
hellaswag = load_dataset('Rowan/hellaswag')
```

**Why Demo Benchmarks?**

- ‚úÖ Fast development iteration
- ‚úÖ Zero external dependencies
- ‚úÖ Demonstrates evaluation patterns
- ‚ö†Ô∏è **NOT for research or production comparison**

For rigorous evaluation: integrate real datasets or use [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

## üõ†Ô∏è Tech Stack

- **Python 3.11+** - Modern Python with type hints
- **Ollama** - Local LLM runtime (no API costs)
- **HuggingFace Datasets** - Standard benchmark datasets
- **scikit-learn** - Statistical metrics and analysis
- **matplotlib** - Static visualizations
- **plotly** - Interactive charts and dashboards
- **seaborn** - Statistical data visualization
- **pytest** - Comprehensive test coverage (87%)

## üìà Performance

Typical evaluation metrics:

- **Speed**: ~100-500ms per prompt (model dependent)
- **Memory**: <2GB RAM for 1B models, <8GB for 7B models
- **Benchmark Time**: ~5-10 minutes per model
- **Cost**: $0 (100% local execution)

## üéì Use Cases

- **Model Selection**: Compare multiple LLMs to choose the best fit for your use case
- **Performance Optimization**: Identify bottlenecks and optimize inference
- **Quality Assurance**: Validate model outputs before production deployment
- **Benchmark Tracking**: Monitor model performance over time
- **Research & Development**: Analyze model behavior across different metrics
- **Cost-Performance Analysis**: Balance quality vs. speed for your requirements

## üìÅ Project Structure

```
llm-evaluation/
‚îú‚îÄ‚îÄ src/llm_evaluator/
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py           # Main evaluation orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py             # Performance & quality metrics
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks.py          # MMLU, TruthfulQA, HellaSwag
‚îÇ   ‚îú‚îÄ‚îÄ visualizations.py      # Charts & dashboards
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Pydantic configuration
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                 # CLI tool
‚îÇ   ‚îî‚îÄ‚îÄ providers/             # Provider implementations
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py        # Base interfaces
‚îÇ       ‚îú‚îÄ‚îÄ ollama_provider.py
‚îÇ       ‚îú‚îÄ‚îÄ openai_provider.py
‚îÇ       ‚îú‚îÄ‚îÄ anthropic_provider.py
‚îÇ       ‚îú‚îÄ‚îÄ huggingface_provider.py
‚îÇ       ‚îî‚îÄ‚îÄ cached_provider.py
‚îú‚îÄ‚îÄ tests/                     # 40 tests, 87% coverage
‚îú‚îÄ‚îÄ examples/                  # 5 runnable demos
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md
‚îÇ   ‚îú‚îÄ‚îÄ FULL_BENCHMARKS.md
‚îÇ   ‚îî‚îÄ‚îÄ LIMITATIONS.md
‚îî‚îÄ‚îÄ pyproject.toml             # v0.2.0
```

## üîó Related Projects

- **[ai-safety-testing](https://github.com/NahuelGiudizi/ai-safety-testing)** - Security vulnerability testing for LLMs

## üìù Development

Built with enterprise best practices:

- ‚úÖ Test-Driven Development (TDD)
- ‚úÖ 87% code coverage
- ‚úÖ Type hints throughout
- ‚úÖ Black code formatting
- ‚úÖ Ruff linting
- ‚úÖ GitHub Actions CI/CD
- ‚úÖ Comprehensive documentation

## ü§ù Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## üìÑ License

MIT License - see LICENSE file for details

## üë§ Author

**Nahuel Giudizi**

- GitHub: [@NahuelGiudizi](https://github.com/NahuelGiudizi)
- LinkedIn: [Nahuel Giudizi](https://www.linkedin.com/in/nahuel-giudizi/)

## üôè Acknowledgments

- [Ollama](https://ollama.com) for local LLM runtime
- [HuggingFace](https://huggingface.co) for datasets and models
- [EleutherAI](https://www.eleuther.ai/) for evaluation frameworks

---

**‚≠ê Star this repo if you find it useful!**
