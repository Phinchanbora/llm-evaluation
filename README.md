# LLM Evaluation Suite

[![Tests](https://github.com/NahuelGiudizi/llm-evaluation/workflows/Tests/badge.svg)](https://github.com/NahuelGiudizi/llm-evaluation/actions)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Coverage](https://img.shields.io/badge/coverage-87%25-brightgreen)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> Enterprise-grade evaluation framework for Large Language Models with comprehensive metrics, benchmarking, and visualization capabilities.

## ğŸ¯ Overview

A production-ready toolkit for evaluating LLM performance across multiple dimensions: quality, performance, and standard benchmarks. Features interactive dashboards, statistical analysis, and seamless integration with Ollama and HuggingFace ecosystems.

## ğŸš€ Features

- **Performance Metrics**: Response time, throughput, token efficiency, latency percentiles
- **Quality Metrics**: Accuracy, coherence, hallucination detection, BLEU scores
- **Standard Benchmarks**: MMLU, TruthfulQA, HellaSwag (âš ï¸ Demo samples - see docs for production datasets)
- **Interactive Visualizations**: Comparison dashboards, radar charts, heatmaps, trend analysis
- **Statistical Analysis**: Significance testing, confidence intervals, distribution analysis
- **Export Capabilities**: HTML reports, PNG charts, JSON data exports
- **Multi-Model Support**: Compare multiple models side-by-side
- **100% Local**: No API costs, complete data privacy with Ollama
- **Clean Architecture**: Provider abstraction, dependency injection, SOLID principles

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/NahuelGiudizi/llm-evaluation.git
cd llm-evaluation

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .
```

## ğŸ”§ Quick Start

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.providers import GenerationConfig

# Configure generation settings
config = GenerationConfig(
    temperature=0.7,
    max_tokens=500,
    timeout_seconds=30,
    retry_attempts=3
)

# Initialize provider with dependency injection
provider = OllamaProvider(model="llama3.2:1b", config=config)

# Create evaluator with provider
evaluator = ModelEvaluator(provider=provider)

# Run comprehensive evaluation
results = evaluator.evaluate_all()

# Print summary
print(f"Accuracy: {results.accuracy:.2%}")
print(f"Avg Response Time: {results.avg_response_time:.2f}s")
print(f"Hallucination Rate: {results.hallucination_rate:.2%}")

# Generate report
evaluator.generate_report(results, output="evaluation_report.md")
```

### ğŸ—ï¸ Architecture Features

**Clean Architecture with Dependency Injection:**
- ğŸ”Œ **Provider Interface** - Swap LLM backends easily (Ollama, OpenAI, Anthropic)
- ğŸ”„ **Retry Logic** - Exponential backoff with configurable attempts
- ğŸ›¡ï¸ **Error Handling** - Comprehensive exception hierarchy
- ğŸ“ **Logging** - Structured logging throughout
- ğŸ¯ **Type Safety** - Strong typing with dataclasses (no `Dict[str, Any]`)
- âœ… **SOLID Principles** - Dependency Inversion, Single Responsibility

## ğŸ“Š Visualization Examples

Generate interactive dashboards with a single function:

```python
from llm_evaluator import quick_comparison

results = {
    "llama3.2:1b": {"mmlu": 0.65, "accuracy": 0.75, "coherence": 0.82},
    "mistral:7b": {"mmlu": 0.78, "accuracy": 0.82, "coherence": 0.88},
    "phi3:3.8b": {"mmlu": 0.71, "accuracy": 0.78, "coherence": 0.85}
}

# Generate all visualizations
quick_comparison(results, output_dir="outputs")
```

Creates:
- ğŸ“Š Bar charts for benchmark comparisons
- ğŸ¯ Radar charts for multi-metric analysis
- ğŸ”¥ Heatmaps for performance overview
- ğŸ“ˆ Line charts for trend analysis
- ğŸ“¦ Box plots for distribution analysis
- ğŸ¨ Interactive HTML dashboards

## ğŸ§ª Benchmarks: Demo vs Production

**âš ï¸ Current Implementation (Demo/POC):**
- MMLU: 3 sample questions
- TruthfulQA: 3 sample questions  
- HellaSwag: 2 sample scenarios

**ğŸ­ Production Datasets (Enterprise Use):**
```python
# Install: pip install datasets
from datasets import load_dataset

# MMLU - 14,042 questions across 57 subjects
mmlu = load_dataset('cais/mmlu', 'all')

# TruthfulQA - 817 factual accuracy questions
truthfulqa = load_dataset('truthful_qa', 'generation')

# HellaSwag - 10,042 commonsense scenarios
hellaswag = load_dataset('Rowan/hellaswag')
```

**Why Demo Benchmarks?**
- âœ… Fast development iteration
- âœ… Zero external dependencies
- âœ… Demonstrates evaluation patterns
- âš ï¸ **NOT for research or production comparison**

For rigorous evaluation: integrate real datasets or use [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

## ğŸ› ï¸ Tech Stack

- **Python 3.11+** - Modern Python with type hints
- **Ollama** - Local LLM runtime (no API costs)
- **HuggingFace Datasets** - Standard benchmark datasets
- **scikit-learn** - Statistical metrics and analysis
- **matplotlib** - Static visualizations
- **plotly** - Interactive charts and dashboards
- **seaborn** - Statistical data visualization
- **pytest** - Comprehensive test coverage (87%)

## ğŸ“ˆ Performance

Typical evaluation metrics:
- **Speed**: ~100-500ms per prompt (model dependent)
- **Memory**: <2GB RAM for 1B models, <8GB for 7B models
- **Benchmark Time**: ~5-10 minutes per model
- **Cost**: $0 (100% local execution)

## ğŸ“ Use Cases

- **Model Selection**: Compare multiple LLMs to choose the best fit for your use case
- **Performance Optimization**: Identify bottlenecks and optimize inference
- **Quality Assurance**: Validate model outputs before production deployment
- **Benchmark Tracking**: Monitor model performance over time
- **Research & Development**: Analyze model behavior across different metrics
- **Cost-Performance Analysis**: Balance quality vs. speed for your requirements

## ğŸ“ Project Structure

```
llm-evaluation/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_evaluator/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ evaluator.py       # Main evaluation orchestrator
â”‚       â”œâ”€â”€ metrics.py          # Performance & quality metrics
â”‚       â”œâ”€â”€ benchmarks.py       # Standard benchmark integrations
â”‚       â””â”€â”€ visualizations.py   # Chart and dashboard generation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py            # Pytest fixtures
â”‚   â”œâ”€â”€ test_metrics.py        # Metrics tests (100% coverage)
â”‚   â”œâ”€â”€ test_evaluator.py      # Evaluator tests
â”‚   â”œâ”€â”€ test_benchmarks.py     # Benchmark tests
â”‚   â””â”€â”€ test_visualizations.py # Visualization tests
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb         # Interactive analysis examples
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ visualizations/        # Generated charts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ EXAMPLES.md            # Usage examples
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yml          # CI/CD pipeline
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ pytest.ini
â””â”€â”€ requirements.txt
```

## ğŸ”— Related Projects

- **[ai-safety-testing](https://github.com/NahuelGiudizi/ai-safety-testing)** - Security vulnerability testing for LLMs

## ğŸ“ Development

Built with enterprise best practices:
- âœ… Test-Driven Development (TDD)
- âœ… 87% code coverage
- âœ… Type hints throughout
- âœ… Black code formatting
- âœ… Ruff linting
- âœ… GitHub Actions CI/CD
- âœ… Comprehensive documentation

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ‘¤ Author

**Nahuel Giudizi**
- GitHub: [@NahuelGiudizi](https://github.com/NahuelGiudizi)
- LinkedIn: [Nahuel Giudizi](https://www.linkedin.com/in/nahuel-giudizi/)

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.com) for local LLM runtime
- [HuggingFace](https://huggingface.co) for datasets and models
- [EleutherAI](https://www.eleuther.ai/) for evaluation frameworks

---

**â­ Star this repo if you find it useful!**
