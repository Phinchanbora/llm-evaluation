# ğŸš€ LLM Benchmark Toolkit

<p align="center">
  <img src="https://img.shields.io/pypi/v/llm-benchmark-toolkit?style=for-the-badge&color=blue" alt="PyPI">
  <img src="https://img.shields.io/pypi/dm/llm-benchmark-toolkit?style=for-the-badge&color=green" alt="Downloads">
  <img src="https://img.shields.io/github/stars/NahuelGiudizi/llm-evaluation?style=for-the-badge" alt="Stars">
  <img src="https://img.shields.io/badge/coverage-79%25-brightgreen?style=for-the-badge" alt="Coverage">
  <img src="https://img.shields.io/badge/python-3.11+-blue?style=for-the-badge" alt="Python">
  <img src="https://img.shields.io/badge/license-MIT-green?style=for-the-badge" alt="License">
</p>

<p align="center">
  <b>ğŸ¯ Benchmark LLMs with 9 benchmarks & 100,000+ real questions</b><br>
  <sub>MMLU â€¢ TruthfulQA â€¢ HellaSwag â€¢ ARC â€¢ WinoGrande â€¢ CommonsenseQA â€¢ BoolQ â€¢ SafetyBench â€¢ Do-Not-Answer</sub>
</p>

<p align="center">
  <a href="#-get-started-60-seconds">Get Started</a> â€¢
  <a href="#-compare-models">Compare Models</a> â€¢
  <a href="#-python-api">Python API</a> â€¢
  <a href="#-academic-use">Academic</a> â€¢
  <a href="#-contributing">Contributing</a>
</p>

---

## ğŸš€ Get Started (60 Seconds)

### Install

```bash
# Full installation (dashboard + all providers)
pip install llm-benchmark-toolkit
```

That's it! Everything included: Dashboard, OpenAI, Anthropic, Ollama, HuggingFace.

### ğŸŒ Web Dashboard (Recommended!)

The easiest way to evaluate models - a beautiful web interface:

```bash
# Launch the dashboard
python -m llm_evaluator.dashboard
```

Opens your browser to `http://localhost:8888` where you can:

- ğŸš€ **Run evaluations** with real-time progress tracking
- ğŸ“Š **Compare models** with interactive charts
- ğŸ” **Inspect scenarios** - see every question & answer
- ğŸ“ˆ **View history** - track improvements over time
- ğŸ’¾ **Export results** - JSON, charts, reports

### Quick CLI Evaluation

```bash
# Set your API key (Windows)
set OPENAI_API_KEY=sk-...

# Or on Linux/Mac
export OPENAI_API_KEY="sk-..."

# Run quick evaluation
python -m llm_evaluator.cli quick
```

**Output:**

```
ğŸš€ LLM QUICK EVALUATION
==================================================
âœ… Provider: openai (gpt-4o-mini)
âœ… Sample size: 20

ğŸ“Š RESULTS
==================================================
  ğŸ¯ MMLU:       78.5%
  ğŸ¯ TruthfulQA: 71.2%
  ğŸ¯ HellaSwag:  82.4%
  
  ğŸ“ˆ Overall:    77.4%
==================================================
âœ¨ Evaluation complete!
```

**Auto-detection works with:**

- `OPENAI_API_KEY` â†’ GPT-4o-mini
- `ANTHROPIC_API_KEY` â†’ Claude 3.5 Sonnet
- `DEEPSEEK_API_KEY` â†’ DeepSeek-V3
- Ollama running locally â†’ Llama 3.2

---

## ğŸ”„ Compare Models

```bash
llm-eval compare \
  --models gpt-4o-mini,claude-3-5-sonnet \
  --sample-size 100
```

**More examples:**

```bash
# Ollama (local models)
llm-eval quick --model llama3.2:1b

# OpenAI
llm-eval quick --model gpt-4o-mini

# Anthropic
llm-eval run --model claude-3-5-sonnet-20241022 --provider anthropic

# DeepSeek (super affordable!)
llm-eval quick --model deepseek-chat

# Run specific benchmarks (any combination!)
llm-eval benchmark --model gpt-4o --benchmarks mmlu,truthfulqa,arc,safetybench

# Run ALL benchmarks
llm-eval benchmark --model llama3.2:1b --benchmarks mmlu,truthfulqa,hellaswag,arc,winogrande,commonsenseqa,boolq,safetybench,donotanswer

# Full academic evaluation
llm-eval academic --model llama3.2:1b \
  --sample-size 500 \
  --output-latex results.tex
```

---

## ğŸ–¥ï¸ CLI Commands Reference

| Command | Description |
|---------|-------------|
| `llm-eval quick` | ğŸš€ Zero-config evaluation (auto-detects provider) |
| `llm-eval run` | Full evaluation on a single model |
| `llm-eval benchmark` | Run specific benchmarks |
| `llm-eval compare` | Compare multiple models side-by-side |
| `llm-eval vs` | ğŸ¥Š Run same benchmark on multiple models sequentially |
| `llm-eval dashboard` | ğŸŒ Launch web dashboard |
| `llm-eval academic` | ğŸ“ Academic evaluation with statistics |
| `llm-eval export` | ğŸ“¤ Export results (JSON, CSV, LaTeX, BibTeX) |
| `llm-eval providers` | Check available providers status |
| `llm-eval list-runs` | ğŸ“‹ List saved evaluation runs |

### Key Options

```bash
# Common options for most commands
-m, --model TEXT       # Model name
-p, --provider TYPE    # ollama, openai, anthropic, huggingface, deepseek
-s, --sample-size INT  # Number of questions to test
-u, --base-url URL     # Custom API endpoint (vLLM, LM Studio, Azure)
--cache / --no-cache   # Enable/disable caching

# Benchmark selection
-b, --benchmarks TEXT  # Comma-separated: mmlu,truthfulqa,hellaswag,arc,
                       # winogrande,commonsenseqa,boolq,safetybench,donotanswer
```

### VS Command (Model Battle)

Compare models head-to-head:

```bash
# Compare two local models
llm-eval vs llama3.2:1b mistral:7b

# Compare with specific benchmarks
llm-eval vs llama3.2:1b mistral:7b -b mmlu,arc -s 50

# Compare models from different providers
llm-eval vs gpt-4o-mini claude-3.5-sonnet -p openai,anthropic
```

---

## ğŸ Python API

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers import OpenAIProvider

provider = OpenAIProvider(model="gpt-4o-mini")
evaluator = ModelEvaluator(provider=provider)

results = evaluator.evaluate_all()
print(f"Overall: {results.overall_score:.1%}")
```

**With caching (10x faster):**

```python
from llm_evaluator.providers import CachedProvider, OllamaProvider

provider = OllamaProvider(model="llama3.2:1b")
cached = CachedProvider(provider)  # Automatic caching!

evaluator = ModelEvaluator(provider=cached)
results = evaluator.evaluate_all()
```

---

## ğŸ¯ Features

| Feature | Description |
|---------|-------------|
| ğŸ“Š **9 Benchmarks** | MMLU, TruthfulQA, HellaSwag, ARC, WinoGrande, CommonsenseQA, BoolQ, SafetyBench, Do-Not-Answer |
| ğŸ”¢ **100,000+ Questions** | Real academic datasets from HuggingFace |
| ğŸ”Œ **5 Providers** | Ollama, OpenAI, Anthropic, DeepSeek, HuggingFace |
| ğŸŒ **Web Dashboard** | Beautiful UI with real-time progress, charts, and history |
| âš¡ **Zero Config** | Auto-detects provider from environment variables |
| ğŸ’¾ **Smart Caching** | 10x faster repeated evaluations |
| ğŸ“ˆ **Academic Rigor** | 95% CI, McNemar tests, baseline comparisons |
| ğŸ“„ **Paper Exports** | LaTeX tables, BibTeX citations, CSV, JSON |
| ğŸ›¡ï¸ **Safety Testing** | SafetyBench + Do-Not-Answer for security evaluation |
| ğŸ¨ **Beautiful CLI** | Progress bars, colored output, ETA tracking |

---

## ğŸ“ Academic Use

For publication-quality evaluations:

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers import OllamaProvider
from llm_evaluator.export import export_to_latex, generate_bibtex

provider = OllamaProvider(model="llama3.2:1b")
evaluator = ModelEvaluator(provider=provider)

results = evaluator.evaluate_all_academic(
    sample_size=500,
    compare_baselines=True
)

# 95% confidence intervals
print(f"MMLU: {results.mmlu_accuracy:.1%}")
print(f"95% CI: [{results.mmlu_ci[0]:.1%}, {results.mmlu_ci[1]:.1%}]")

# Compare to GPT-4, Claude, Llama baselines
for baseline, comparison in results.baseline_comparison.items():
    print(f"vs {baseline}: {comparison['difference']:+.1%}")

# Export for papers
latex = export_to_latex(results, "My Model")
bibtex = generate_bibtex()
```

---

## ğŸ¨ Visual Output Examples

### Benchmark Comparison

![Benchmark Comparison](docs/images/benchmark_comparison.png)

### Interactive Dashboard

![Dashboard](docs/images/dashboard.png)

*(Add screenshots to `docs/images/` folder)*

---

## ğŸ”Œ Check Available Providers

```bash
llm-eval providers
```

```
ğŸ”Œ Available Providers:

âœ… Auto-detected: openai (gpt-4o-mini)

  âœ… ollama          - Local LLMs (llama3.2, mistral, etc.)
  âœ… openai          - GPT-3.5, GPT-4, GPT-4o
  âŒ anthropic       - Claude 3/3.5 (pip install anthropic)
  âœ… deepseek        - DeepSeek-V3, DeepSeek-R1
  âŒ huggingface     - Inference API

ğŸ“‹ Environment Variables:
  âœ… OPENAI_API_KEY       sk-abc1...
  âŒ ANTHROPIC_API_KEY    Not set
```

---

## ğŸ”¬ Benchmarks Included

### ğŸ“š Knowledge & Reasoning (7 benchmarks)

| Benchmark | Questions | Description |
|-----------|-----------|-------------|
| **MMLU** | 14,042 | Massive Multitask Language Understanding - 57 subjects |
| **TruthfulQA** | 817 | Truthfulness and avoiding misinformation |
| **HellaSwag** | 10,042 | Common-sense reasoning and sentence completion |
| **ARC-Challenge** | 2,590 | Grade-school science questions (hard subset) |
| **WinoGrande** | 44,000 | Pronoun resolution and commonsense reasoning |
| **CommonsenseQA** | 12,247 | Commonsense knowledge questions |
| **BoolQ** | 15,942 | Yes/no reading comprehension questions |

### ğŸ›¡ï¸ Safety & Security (2 benchmarks)

| Benchmark | Questions | Description |
|-----------|-----------|-------------|
| **SafetyBench** | 11,000 | Safety evaluation across multiple risk categories |
| **Do-Not-Answer** | 939 | Harmful prompt detection and refusal testing |

**Total: 9 benchmarks, 100,000+ questions**

---

## ğŸ¤ Contributing

This is open source. Make it better:

```bash
git clone https://github.com/NahuelGiudizi/llm-evaluation
cd llm-evaluation
pip install -e ".[dev]"
pytest tests/ -v
```

### Wanted

- [ ] More providers (Cohere, AI21, Groq, Together.ai)
- [ ] More benchmarks (GSM8K, HumanEval, GPQA, MT-Bench)
- [ ] Async evaluation for faster throughput
- [ ] Batch evaluation mode
- [ ] Custom benchmark support
- [ ] Docker image for easy deployment

**Contributors welcome!** ğŸ‰

---

## ğŸ“š Documentation

| Doc | Description |
|-----|-------------|
| ğŸ“– [Quick Start](docs/QUICKSTART.md) | Get running in 5 minutes |
| ğŸ”Œ [Providers Guide](docs/PROVIDERS.md) | Ollama, OpenAI, Anthropic, DeepSeek, HuggingFace |
| ğŸ”¬ [Benchmarks](docs/FULL_BENCHMARKS.md) | MMLU, TruthfulQA, HellaSwag details |
| ğŸ“ [Academic Usage](docs/ACADEMIC_USAGE.md) | Statistical methods, LaTeX export |
| ğŸ“˜ [API Reference](docs/API.md) | Complete Python API documentation |

---

## ğŸ“Š Output Formats

```bash
# JSON (default)
llm-eval run --model llama3.2:1b --output results.json

# Export to multiple formats
llm-eval export results.json --format all

# Individual formats
llm-eval export results.json --format csv
llm-eval export results.json --format latex
llm-eval export results.json --format bibtex

# Academic evaluation with direct exports
llm-eval academic --model llama3.2:1b --output-latex table.tex --output-bibtex refs.bib
```

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

---

## â­ Star History

If this project helped you, please star it! â­

[![Star History Chart](https://api.star-history.com/svg?repos=NahuelGiudizi/llm-evaluation&type=Date)](https://star-history.com/#NahuelGiudizi/llm-evaluation&Date)

---

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/NahuelGiudizi">Nahuel Giudizi</a>
</p>

<p align="center">
  <a href="https://pypi.org/project/llm-benchmark-toolkit/">
    <img src="https://img.shields.io/badge/Install-pip%20install%20llm--benchmark--toolkit-blue?style=for-the-badge&logo=python" alt="Install">
  </a>
</p>
