# üöÄ Tutorial R√°pido - LLM Evaluation

## ¬øQu√© es este proyecto?

Un framework para **evaluar modelos de lenguaje** con benchmarks **reales** (24,901 preguntas) y generar **gr√°ficos comparativos**.

---

## üìã Paso a Paso - Tu Primera Evaluaci√≥n

### 1Ô∏è‚É£ Activar el entorno virtual

```powershell
cd "c:\Users\nahue\Desktop\Python Projects\llm-evaluation"
.\venv\Scripts\activate
```

### 2Ô∏è‚É£ Verificar que Ollama est√° corriendo

Abre otra terminal y ejecuta:
```powershell
ollama list
```

Deber√≠as ver tus modelos (llama3.2:1b, mistral, etc.)

### 3Ô∏è‚É£ Ejecutar tu primera evaluaci√≥n (30 segundos)

```powershell
python examples/demo.py
```

**Esto eval√∫a tu modelo con 8 preguntas de prueba** (modo demo, s√∫per r√°pido).

**SALIDA:**
```
MMLU: 62.5%
TruthfulQA: 75.0%
HellaSwag: 50.0%
Overall Score: 62.5%
```

---

## üìä Ver Gr√°ficos - 3 Formas

### Opci√≥n 1: Demo de Visualizaciones (RECOMENDADO para empezar)

```powershell
python examples/demo_visualizations.py
```

**Esto genera 7 gr√°ficos en `outputs/visualizations/`:**
- `benchmark_comparison.png` - Comparaci√≥n de benchmarks (barras)
- `benchmark_interactive.html` - **ABRIR ESTE** en el navegador (interactivo)
- `quality_radar.html` - Radar chart de calidad
- `performance_heatmap.png` - Mapa de calor
- `score_distribution.html` - Distribuci√≥n de scores
- `performance_trends.html` - Tendencias de performance
- `dashboard.html` - **DASHBOARD COMPLETO** con todos los gr√°ficos

**ABRE:** `outputs/visualizations/dashboard.html` en tu navegador üéâ

---

### Opci√≥n 2: Comparar 2+ Modelos Reales (con evaluaci√≥n real)

```powershell
python examples/compare_models.py
```

**Esto eval√∫a 2 modelos de Ollama** (llama3.2:1b vs mistral:7b) y genera gr√°ficos en `outputs/comparisons/`.

**Tiempo:** ~2 minutos (usa 8 preguntas demo por modelo).

---

### Opci√≥n 3: CLI Tool (m√°s flexible)

```powershell
# Evaluar 1 modelo
llm-eval run --model llama3.2:1b

# Comparar 2 modelos
llm-eval compare --models llama3.2:1b,mistral:7b

# Benchmark espec√≠fico (MMLU con 100 preguntas)
llm-eval benchmark --model llama3.2:1b --benchmark mmlu --sample-size 100
```

---

## üéØ Workflow Recomendado

### Para entender el proyecto (10 minutos):

1. **Ver datos simulados** (sin evaluaci√≥n real):
   ```powershell
   python examples/demo_visualizations.py
   ```
   üìÇ Abre: `outputs/visualizations/dashboard.html`

2. **Comparar modelos reales** (evaluaci√≥n real):
   ```powershell
   python examples/compare_models.py
   ```
   üìÇ Abre: `outputs/comparisons/comparison_dashboard.html`

---

## üìÅ Estructura del Proyecto

```
llm-evaluation/
‚îú‚îÄ‚îÄ src/llm_evaluator/
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py          # Motor principal de evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks.py          # MMLU, TruthfulQA, HellaSwag
‚îÇ   ‚îú‚îÄ‚îÄ visualizations.py      # Generador de gr√°ficos
‚îÇ   ‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_provider.py      # Ollama (local)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py      # GPT-3.5/4
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_provider.py   # Claude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ huggingface_provider.py # HuggingFace
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cached_provider.py      # Cache (10x speedup)
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                 # Comando llm-eval
‚îÇ
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ demo.py                      # ‚Üê EMPIEZA AQU√ç (30s)
‚îÇ   ‚îú‚îÄ‚îÄ demo_visualizations.py       # ‚Üê VER GR√ÅFICOS (datos simulados)
‚îÇ   ‚îú‚îÄ‚îÄ compare_models.py            # ‚Üê COMPARAR MODELOS REALES
‚îÇ   ‚îú‚îÄ‚îÄ demo_full_benchmarks.py      # Full benchmarks (8 horas)
‚îÇ   ‚îî‚îÄ‚îÄ demo_vs_real.py              # Demo vs Real comparison
‚îÇ
‚îú‚îÄ‚îÄ outputs/                   # ‚Üê AQU√ç SE GUARDAN LOS GR√ÅFICOS
‚îÇ   ‚îú‚îÄ‚îÄ visualizations/        # Gr√°ficos de demo
‚îÇ   ‚îî‚îÄ‚îÄ comparisons/           # Comparaciones reales
‚îÇ
‚îî‚îÄ‚îÄ tests/                     # 58 tests (87% coverage)
```

---

## üîß Configuraci√≥n de Providers

### Ollama (Local) - Ya funciona ‚úÖ
No necesitas configurar nada, usa tu Ollama local.

### OpenAI (GPT-4) - Opcional
```powershell
# Crea .env en la ra√≠z del proyecto
echo "OPENAI_API_KEY=tu-api-key-aqui" > .env

# Instala dependencias
pip install -e ".[openai]"

# Usa en Python
from llm_evaluator.providers.openai_provider import OpenAIProvider
provider = OpenAIProvider(model="gpt-3.5-turbo")
```

### Anthropic (Claude) - Opcional
```powershell
echo "ANTHROPIC_API_KEY=tu-api-key-aqui" > .env
pip install -e ".[anthropic]"
```

### HuggingFace - Opcional
```powershell
echo "HUGGINGFACE_API_KEY=tu-api-key-aqui" > .env
pip install -e ".[huggingface]"
```

---

## üé® Tipos de Gr√°ficos Disponibles

1. **Benchmark Comparison** (barras) - Compara MMLU, TruthfulQA, HellaSwag
2. **Radar Chart** (pol√≠gono) - Accuracy, Coherence, Consistency
3. **Heatmap** (mapa de calor) - Performance de m√∫ltiples modelos
4. **Score Distribution** (boxplot) - Distribuci√≥n de scores
5. **Performance Trends** (l√≠neas) - Evoluci√≥n temporal
6. **Dashboard** (todo junto) - Vista completa interactiva

---

## üí° Ejemplos de C√≥digo

### Ejemplo 1: Evaluaci√≥n b√°sica con gr√°ficos

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.visualizations import EvaluationVisualizer

# Evaluar modelo
provider = OllamaProvider(model="llama3.2:1b")
evaluator = ModelEvaluator(provider=provider)
results = evaluator.evaluate_all()

# Generar gr√°ficos
viz = EvaluationVisualizer()
results_dict = {
    "llama3.2:1b": {
        "mmlu": results.quality_metrics.get("mmlu_score", 0),
        "truthful_qa": results.quality_metrics.get("truthful_qa_score", 0),
        "hellaswag": results.quality_metrics.get("hellaswag_score", 0),
    }
}

viz.plot_benchmark_comparison(
    results_dict,
    output_path="my_benchmark.html",
    interactive=True
)

print(f"‚úÖ Gr√°fico guardado: my_benchmark.html")
```

### Ejemplo 2: Comparar 3 modelos

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.visualizations import quick_comparison

models = ["llama3.2:1b", "mistral:7b", "phi3:3.8b"]
results = {}

for model in models:
    provider = OllamaProvider(model=model)
    evaluator = ModelEvaluator(provider=provider)
    results[model] = evaluator.evaluate_all()

# Genera dashboard completo
quick_comparison(results, output_dir="my_comparison")
print(f"‚úÖ Dashboard: my_comparison/comparison_dashboard.html")
```

### Ejemplo 3: Usar caching (10x m√°s r√°pido)

```python
from llm_evaluator import ModelEvaluator
from llm_evaluator.providers.ollama_provider import OllamaProvider
from llm_evaluator.providers.cached_provider import CachedProvider

# Wrap el provider con cache
base_provider = OllamaProvider(model="llama3.2:1b")
cached_provider = CachedProvider(base_provider, ttl=3600)  # 1 hora

evaluator = ModelEvaluator(provider=cached_provider)

# Primera evaluaci√≥n: lenta (llama al modelo)
results1 = evaluator.evaluate_all()  # ~60 segundos

# Segunda evaluaci√≥n: r√°pida (usa cache)
results2 = evaluator.evaluate_all()  # ~6 segundos (10x m√°s r√°pido)

# Ver estad√≠sticas de cache
stats = cached_provider.get_cache_stats()
print(f"Cache hits: {stats['hits']}, Misses: {stats['misses']}")
print(f"Hit rate: {stats['hit_rate_percent']:.1f}%")
```

---

## üêõ Troubleshooting

### "No module named 'ollama'"
```powershell
pip install ollama
```

### "Connection refused" al evaluar
Verifica que Ollama est√° corriendo:
```powershell
ollama serve
```

### "Model not found"
Descarga el modelo:
```powershell
ollama pull llama3.2:1b
```

### No veo los gr√°ficos
Los gr√°ficos se guardan en `outputs/`. Busca archivos `.html` y `.png`:
```powershell
Get-ChildItem -Path outputs -Recurse -Include *.html,*.png
```

---

## üìö Modos de Evaluaci√≥n

| Modo | Preguntas | Tiempo | Uso |
|------|-----------|--------|-----|
| **Demo** | 8 | 30s | Testing r√°pido |
| **Sample** | 100-500 | 5min | Evaluaci√≥n media |
| **Full** | 24,901 | 8hrs | Paper cient√≠fico |

```python
# Demo mode (default)
evaluator = ModelEvaluator(provider=provider)
results = evaluator.evaluate_all()  # 8 preguntas

# Sample mode
from llm_evaluator.benchmarks import BenchmarkRunner
runner = BenchmarkRunner(provider=provider, mode="full", sample_size=100)
results = runner.run_all_benchmarks()  # 100 preguntas

# Full mode (cuidado, 8 horas)
runner = BenchmarkRunner(provider=provider, mode="full")
results = runner.run_all_benchmarks()  # 24,901 preguntas
```

---

## üéØ Pr√≥ximos Pasos

1. ‚úÖ **Ejecuta `python examples/demo_visualizations.py`**
2. ‚úÖ **Abre `outputs/visualizations/dashboard.html`**
3. üîÑ **Ejecuta `python examples/compare_models.py`** (evaluaci√≥n real)
4. üìä **Abre `outputs/comparisons/comparison_dashboard.html`**
5. üöÄ **Usa el CLI**: `llm-eval run --model tu-modelo`

---

## üìû Ayuda

- **Docs completas**: `docs/QUICKSTART.md`
- **Tests**: `pytest tests/ -v`
- **GitHub**: https://github.com/NahuelGiudizi/llm-evaluation

---

## üéâ Resumen Visual

```
TU WORKFLOW:
1. python examples/demo_visualizations.py     ‚Üí Ver gr√°ficos simulados
2. Abrir: outputs/visualizations/dashboard.html ‚Üí üé® Dashboard interactivo
3. python examples/compare_models.py          ‚Üí Evaluar modelos reales
4. Abrir: outputs/comparisons/comparison_dashboard.html ‚Üí üìä Comparaci√≥n real
```

**¬°Disfruta evaluando tus LLMs! üöÄ**
